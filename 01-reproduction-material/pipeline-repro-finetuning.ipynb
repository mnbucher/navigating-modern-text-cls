{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9dff67f-6738-4d04-9f98-86191439fa77",
   "metadata": {},
   "source": [
    "# Replication Pipeline for Fine-Tuning Results\n",
    "\n",
    "This notebook contains the code to reproduce the fine-tuned LLM results reported in our paper. To run the code, execute all code blocks. Because for each dataset each model is trained three times using different random seeds, execution will take a considerable amount of time. See our note on Google Colab in the accompanying `README` file for an option to speed up execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b23ad",
   "metadata": {},
   "source": [
    "### Preliminaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955494fc-2154-48e7-9d50-07afc80ebaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (only required if not already installed)\n",
    "# !pip install sentencepiece\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install wandb\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch\n",
    "# !pip install torchmetrics\n",
    "# !pip install transformers\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313b178-fadd-410f-9f08-959de6a51268",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "# Import standard Python packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Import deep learning packages\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "import wandb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "\n",
    "# Import pipeline code\n",
    "from src.finetuning import train_and_predict_test, init_model, predict_y_from_trained_model, read_x_from_csv, init_misc, compute_and_print_metrics_for_dataset_b, set_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f5814",
   "metadata": {},
   "source": [
    "### Loop through data sets and models to create results for tables 1-4\n",
    "\n",
    "To run the code selectively for individual datasets and/or models, adjust the `range()` arguments for the `ds` and `lm` iterators. For example to run the code for only `DATASET = \"02-twitter-stance\"` and `LANGUAGE_MODEL = \"DEB-V3\"`, set `for ds in range(2,3)` and `for lm in range(3,4)`.\n",
    "\n",
    "To run the German language model for case study 3, switch from `dataset_sentences = f\"./data/{DATASET}/all-x-translated.csv\"` to the non-translated dataset (in the preceeding line of code), switch to `LANGUAGE_FOR_MODEL=\"de\"`, and activate `LANGUAGE_MODEL = \"ELE-BS-GER\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836716c-b49c-488e-b338-c5eb952108ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through datasets\n",
    "for ds in range(1,5):\n",
    "\n",
    "    # ************************************************ #\n",
    "    # Choose dataset for reproduction of paper results\n",
    "    # ************************************************ #\n",
    "\n",
    "    # case study 1\n",
    "    if ds == 1:\n",
    "        DATASET = \"01-nyt-sentiment\"\n",
    "        dataset_sentences = f\"./data/{DATASET}/all-x.csv\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    # case study 2\n",
    "    if ds == 2:\n",
    "        DATASET = \"02-twitter-stance\"\n",
    "        dataset_sentences = f\"./data/{DATASET}/all-x.csv\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    # case study 3\n",
    "    if ds == 3:\n",
    "        DATASET = \"03-emotion-angry\"\n",
    "        #dataset_sentences = f\"./data/{DATASET}/all-x.csv\"\n",
    "        dataset_sentences = f\"./data/{DATASET}/all-x-translated.csv\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    # case study 4\n",
    "    if ds == 4:\n",
    "        DATASET = \"04-brexit-stance\"\n",
    "        dataset_sentences = f\"./data/{DATASET}/all-x.csv\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    LANGUAGE_FOR_MODEL=\"en\"\n",
    "    #LANGUAGE_FOR_MODEL=\"de\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    # Loop through language models\n",
    "    for lm in range(1,6):\n",
    "\n",
    "        if lm == 1: LANGUAGE_MODEL = \"ROB-BASE\"\n",
    "        # https://huggingface.co/roberta-base\n",
    "\n",
    "        if lm == 2: LANGUAGE_MODEL = \"ROB-LRG\"\n",
    "        # https://huggingface.co/roberta-large\n",
    "\n",
    "        if lm == 3: LANGUAGE_MODEL = \"DEB-V3\"\n",
    "        # https://huggingface.co/microsoft/deberta-v3-large\n",
    "\n",
    "        if lm == 4: LANGUAGE_MODEL = \"ELE-LRG\"\n",
    "        # https://huggingface.co/google/electra-large-discriminator\n",
    "\n",
    "        if lm == 5: LANGUAGE_MODEL = \"XLNET-LRG\"\n",
    "        # https://huggingface.co/xlnet-large-cased\n",
    "\n",
    "        # LANGUAGE_MODEL = \"ELE-BS-GER\"\n",
    "        # To use the electra base model in german,\n",
    "        # set LANGUAGE_FOR_MODEL=\"de\" above.\n",
    "        # Also choose non-translated data for # case study 3.\n",
    "        # https://huggingface.co/german-nlp-group/electra-base-german-uncased\n",
    "\n",
    "        # For BART and ChatGPT, see separate notebooks.\n",
    "\n",
    "        # To use a custom model from Huggingface,\n",
    "        # set the model ID with the following variable:\n",
    "\n",
    "        CUSTOM_MODEL_NAME = None\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        RUN_ID = DATASET + \"-\" + LANGUAGE_MODEL + \"-\" + LANGUAGE_FOR_MODEL\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        RAND_SEED = 1234\n",
    "\n",
    "        N_EPOCHS = 10\n",
    "\n",
    "        if lm == 3:\n",
    "            BATCH_SIZE = 2\n",
    "        else:\n",
    "            BATCH_SIZE = 4\n",
    "\n",
    "        GRADIENT_ACC_STEPS = 8\n",
    "\n",
    "        DROPOUT_RATE = 0.1\n",
    "\n",
    "        LEARNING_RATE = 1e-5\n",
    "\n",
    "        IMBALANCE_STRATEGY = 'loss_weight'\n",
    "\n",
    "        IS_DEBUG_ENABLED = True\n",
    "\n",
    "        DO_VALIDATION_SET = False\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        dataset_labels = f\"./data/{DATASET}/all-y.csv\"\n",
    "\n",
    "        all_x = np.squeeze(np.array(pd.read_csv(dataset_sentences, header=None, sep='\\t\\t', engine='python')))\n",
    "        all_y = np.squeeze(np.array(pd.read_csv(dataset_labels, dtype=np.float32, header=None)))\n",
    "\n",
    "        os.makedirs(f'./data/{DATASET}/{RUN_ID}', exist_ok=True)\n",
    "\n",
    "        print(all_x.shape, all_y.shape)\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        # Option 1: no external logging of the training metrics\n",
    "        IS_LOGGING_ENABLED = False\n",
    "        wandb_config = None\n",
    "\n",
    "        # Option 2: external logging of the training metrics (for finetuned analysis and optimization of hyperparameters)\n",
    "        # IS_LOGGING_ENABLED = True\n",
    "        # wandb_config = { \"project\": \"ipz-nlp\", \"entity\": \"mnbucher\" }\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        # random shuffling of loaded dataset\n",
    "        set_seeds(RAND_SEED)\n",
    "        idxs_shuffle = np.arange(all_x.shape[0])\n",
    "        np.random.shuffle(idxs_shuffle)\n",
    "        all_x = all_x[idxs_shuffle]\n",
    "        all_y = all_y[idxs_shuffle]\n",
    "\n",
    "        # test set - fix split with N=200\n",
    "        N_SUBSET_FOR_B = 200\n",
    "        mask_b = np.zeros(all_x.shape[0], dtype=bool)\n",
    "        idxs_b = np.random.choice(np.arange(all_x.shape[0]), replace=False, size=N_SUBSET_FOR_B)\n",
    "        mask_b[idxs_b] = True\n",
    "        dataset_B_unlabelled_x = all_x[mask_b]\n",
    "        dataset_B_unlabelled_y = all_y[mask_b]\n",
    "        np.savetxt(f\"./data/{DATASET}/{RUN_ID}/dataset-b-x.csv\", dataset_B_unlabelled_x, \"%s\", encoding=\"utf-8\") ### NEW\n",
    "        np.savetxt(f\"./data/{DATASET}/{RUN_ID}/dataset-b-y-true.csv\", dataset_B_unlabelled_y, encoding=\"utf-8\") ### NEW\n",
    "\n",
    "        # training set\n",
    "        mask_a = np.ones(all_x.shape[0], dtype=bool)\n",
    "        mask_a[idxs_b] = False\n",
    "        dataset_A_labelled_x = all_x[mask_a]\n",
    "        dataset_A_labelled_y = all_y[mask_a]\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"dataset full size: \", all_x.shape)\n",
    "        print(\"\")\n",
    "        print(\"dataset_B_unlabelled_x: \", dataset_B_unlabelled_x.shape)\n",
    "        print(\"dataset_B_unlabelled_y: \", dataset_B_unlabelled_y.shape)\n",
    "        print(\"\")\n",
    "        print(\"dataset_A_labelled_x: \", dataset_A_labelled_x.shape)\n",
    "        print(\"dataset_A_labelled_y: \", dataset_A_labelled_y.shape)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        ## A: Full Training Datasets\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        RAND_SEED = 1234\n",
    "        set_seeds(RAND_SEED)\n",
    "        dataset_B_unlabelled_y_preds_seed_1234 = train_and_predict_test(dataset_A_labelled_x, dataset_A_labelled_y, RUN_ID, N_EPOCHS, IMBALANCE_STRATEGY, dataset_B_unlabelled_x, learning_rate=LEARNING_RATE, dropout_rate=DROPOUT_RATE, batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACC_STEPS, rand_seed=RAND_SEED, language_model=LANGUAGE_MODEL, language_for_model=LANGUAGE_FOR_MODEL, custom_model_name=CUSTOM_MODEL_NAME, do_validation_set=DO_VALIDATION_SET, log_with_wandb=IS_LOGGING_ENABLED, is_debug=IS_DEBUG_ENABLED, wandb_config=wandb_config)\n",
    "        np.savetxt(f\"y_pred_1234.csv\", dataset_B_unlabelled_y_preds_seed_1234, delimiter=\",\")\n",
    "        np.savetxt(f\"./output/{RUN_ID}_predictions-1234.csv\", dataset_B_unlabelled_y_preds_seed_1234, fmt='%f')\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        RAND_SEED = 3456\n",
    "        set_seeds(RAND_SEED)\n",
    "        dataset_B_unlabelled_y_preds_seed_3456 = train_and_predict_test(dataset_A_labelled_x, dataset_A_labelled_y, RUN_ID, N_EPOCHS, IMBALANCE_STRATEGY, dataset_B_unlabelled_x, learning_rate=LEARNING_RATE, dropout_rate=DROPOUT_RATE, batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACC_STEPS, rand_seed=RAND_SEED, language_model=LANGUAGE_MODEL, language_for_model=LANGUAGE_FOR_MODEL, custom_model_name=CUSTOM_MODEL_NAME, do_validation_set=DO_VALIDATION_SET, log_with_wandb=IS_LOGGING_ENABLED, is_debug=IS_DEBUG_ENABLED, wandb_config=wandb_config)\n",
    "        np.savetxt(f\"y_pred_3456.csv\", dataset_B_unlabelled_y_preds_seed_3456, delimiter=\",\")\n",
    "        np.savetxt(f\"./output/{RUN_ID}_predictions-3456.csv\", dataset_B_unlabelled_y_preds_seed_3456, fmt='%f')\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        RAND_SEED = 5678\n",
    "        set_seeds(RAND_SEED)\n",
    "        dataset_B_unlabelled_y_preds_seed_5678 = train_and_predict_test(dataset_A_labelled_x, dataset_A_labelled_y, RUN_ID, N_EPOCHS, IMBALANCE_STRATEGY, dataset_B_unlabelled_x, learning_rate=LEARNING_RATE, dropout_rate=DROPOUT_RATE, batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACC_STEPS, rand_seed=RAND_SEED, language_model=LANGUAGE_MODEL, language_for_model=LANGUAGE_FOR_MODEL, custom_model_name=CUSTOM_MODEL_NAME, do_validation_set=DO_VALIDATION_SET, log_with_wandb=IS_LOGGING_ENABLED, is_debug=IS_DEBUG_ENABLED, wandb_config=wandb_config)\n",
    "        np.savetxt(f\"y_pred_5678.csv\", dataset_B_unlabelled_y_preds_seed_5678, delimiter=\",\")\n",
    "        np.savetxt(f\"./output/{RUN_ID}_predictions-5678.csv\", dataset_B_unlabelled_y_preds_seed_5678, fmt='%f')\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        y_pred_1234 = np.squeeze(np.asarray(pd.read_csv('y_pred_1234.csv', sep=',', header=None)))\n",
    "        y_pred_3456 = np.squeeze(np.asarray(pd.read_csv('y_pred_3456.csv', sep=',', header=None)))\n",
    "        y_pred_5678 = np.squeeze(np.asarray(pd.read_csv('y_pred_5678.csv', sep=',', header=None)))\n",
    "\n",
    "        print(dataset_B_unlabelled_y.shape, y_pred_1234.shape, y_pred_3456.shape, y_pred_5678.shape)\n",
    "\n",
    "        y_preds = [ y_pred_1234, y_pred_3456, y_pred_5678 ]\n",
    "\n",
    "        compute_and_print_metrics_for_dataset_b(dataset_B_unlabelled_y, y_preds, None, RAND_SEED, False, True, dataset_name = RUN_ID)\n",
    "\n",
    "        # clean up\n",
    "        files = [ f for f in glob.glob('./output/*.csv') ] \n",
    "        files_dest = [ f.replace(\"/output/\", \"/results/\") for f in glob.glob('./output/*.csv') ]\n",
    "        for f in range(len(files)):\n",
    "            shutil.move(files[f], files_dest[f])\n",
    "\n",
    "        files_r1 = [ f for f in glob.glob('./*.csv') ]\n",
    "        files_r2 = [ f for f in glob.glob('./*.txt') ]\n",
    "        files_r = files_r1 + files_r2\n",
    "        for f in files_r:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70eed70",
   "metadata": {},
   "source": [
    "### Create results for section \"Fine-Tuning: The Effect of Training Set Size on Model Performance\"\n",
    "\n",
    "The following code loops through all dataset and produces results for `LANGUAGE_MODEL = \"ROB-LRG\"` based on varying training dataset sizes (50, 100, 200, 500, 1000). We preserve the option to activate any of the other models via the range arguments for `lm`. \n",
    "\n",
    "Also see the notebook `plot_ablation_study.ipynb` in the `figures` folder, which creates the plots in Figure 4 based on the results of the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through datasets\n",
    "for ds in range(1,5):\n",
    "\n",
    "    # ************************************************ #\n",
    "    # Choose dataset for reproduction of paper results\n",
    "    # ************************************************ #\n",
    "\n",
    "    # case study 1\n",
    "    if ds == 1:\n",
    "        DATASET = \"01-nyt-sentiment\"\n",
    "        dataset_sentences = f\"./data/{DATASET}/all-x.csv\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    # case study 2\n",
    "    if ds == 2:\n",
    "        DATASET = \"02-twitter-stance\"\n",
    "        dataset_sentences = f\"./data/{DATASET}/all-x.csv\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    # case study 3\n",
    "    if ds == 3:\n",
    "        DATASET = \"03-emotion-angry\"\n",
    "        #dataset_sentences = f\"./data/{DATASET}/all-x.csv\"\n",
    "        dataset_sentences = f\"./data/{DATASET}/all-x-translated.csv\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    # case study 4\n",
    "    if ds == 4:\n",
    "        DATASET = \"04-brexit-stance\"\n",
    "        dataset_sentences = f\"./data/{DATASET}/all-x.csv\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    LANGUAGE_FOR_MODEL=\"en\"\n",
    "    #LANGUAGE_FOR_MODEL=\"de\"\n",
    "\n",
    "    # ************************************************ #\n",
    "\n",
    "    # Loop through language models\n",
    "    for lm in range(2,3):\n",
    "\n",
    "        if lm == 1: LANGUAGE_MODEL = \"ROB-BASE\"\n",
    "        # https://huggingface.co/roberta-base\n",
    "\n",
    "        if lm == 2: LANGUAGE_MODEL = \"ROB-LRG\"\n",
    "        # https://huggingface.co/roberta-large\n",
    "\n",
    "        if lm == 3: LANGUAGE_MODEL = \"DEB-V3\"\n",
    "        # https://huggingface.co/microsoft/deberta-v3-large\n",
    "\n",
    "        if lm == 4: LANGUAGE_MODEL = \"ELE-LRG\"\n",
    "        # https://huggingface.co/google/electra-large-discriminator\n",
    "\n",
    "        if lm == 5: LANGUAGE_MODEL = \"XLNET-LRG\"\n",
    "        # https://huggingface.co/xlnet-large-cased\n",
    "\n",
    "        # LANGUAGE_MODEL = \"ELE-BS-GER\"\n",
    "        # To use the electra base model in german,\n",
    "        # set LANGUAGE_FOR_MODEL=\"de\" above.\n",
    "        # https://huggingface.co/german-nlp-group/electra-base-german-uncased\n",
    "\n",
    "        # For BART and ChatGPT, see separate notebooks.\n",
    "\n",
    "        # To use a custom model from Huggingface,\n",
    "        # set the model ID with the following variable:\n",
    "\n",
    "        CUSTOM_MODEL_NAME = None\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        RUN_ID = DATASET + \"-\" + LANGUAGE_MODEL + \"-\" + LANGUAGE_FOR_MODEL\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        RAND_SEED = 1234\n",
    "\n",
    "        N_EPOCHS = 10\n",
    "\n",
    "        if lm == 3:\n",
    "            BATCH_SIZE = 2\n",
    "        else:\n",
    "            BATCH_SIZE = 4\n",
    "\n",
    "        GRADIENT_ACC_STEPS = 8\n",
    "\n",
    "        DROPOUT_RATE = 0.1\n",
    "\n",
    "        LEARNING_RATE = 1e-5\n",
    "\n",
    "        IMBALANCE_STRATEGY = 'loss_weight'\n",
    "\n",
    "        IS_DEBUG_ENABLED = True\n",
    "\n",
    "        DO_VALIDATION_SET = False\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        dataset_labels = f\"./data/{DATASET}/all-y.csv\"\n",
    "\n",
    "        all_x = np.squeeze(np.array(pd.read_csv(dataset_sentences, header=None, sep='\\t\\t', engine='python')))\n",
    "        all_y = np.squeeze(np.array(pd.read_csv(dataset_labels, dtype=np.float32, header=None)))\n",
    "\n",
    "        os.makedirs(f'./data/{DATASET}/{RUN_ID}', exist_ok=True)\n",
    "\n",
    "        print(all_x.shape, all_y.shape)\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        # Option 1: no external logging of the training metrics\n",
    "        IS_LOGGING_ENABLED = False\n",
    "        wandb_config = None\n",
    "\n",
    "        # Option 2: external logging of the training metrics (for finetuned analysis and optimization of hyperparameters)\n",
    "        # IS_LOGGING_ENABLED = True\n",
    "        # wandb_config = { \"project\": \"ipz-nlp\", \"entity\": \"mnbucher\" }\n",
    "\n",
    "        # ************************************************ #\n",
    "\n",
    "        # random shuffling of loaded dataset\n",
    "        set_seeds(RAND_SEED)\n",
    "        idxs_shuffle = np.arange(all_x.shape[0])\n",
    "        np.random.shuffle(idxs_shuffle)\n",
    "        all_x = all_x[idxs_shuffle]\n",
    "        all_y = all_y[idxs_shuffle]\n",
    "\n",
    "        # test set\n",
    "        # fix split with N=200\n",
    "        N_SUBSET_FOR_B = 200\n",
    "        mask_b = np.zeros(all_x.shape[0], dtype=bool)\n",
    "        idxs_b = np.random.choice(np.arange(all_x.shape[0]), replace=False, size=N_SUBSET_FOR_B)\n",
    "        mask_b[idxs_b] = True\n",
    "        dataset_B_unlabelled_x = all_x[mask_b]\n",
    "        dataset_B_unlabelled_y = all_y[mask_b]\n",
    "        np.savetxt(f\"./data/{DATASET}/{RUN_ID}/dataset-b-x.csv\", dataset_B_unlabelled_x, \"%s\", encoding=\"utf-8\") ### NEW\n",
    "        np.savetxt(f\"./data/{DATASET}/{RUN_ID}/dataset-b-y-true.csv\", dataset_B_unlabelled_y, encoding=\"utf-8\") ### NEW\n",
    "\n",
    "        # training set\n",
    "        mask_a = np.ones(all_x.shape[0], dtype=bool)\n",
    "        mask_a[idxs_b] = False\n",
    "        dataset_A_labelled_x = all_x[mask_a]\n",
    "        dataset_A_labelled_y = all_y[mask_a]\n",
    "\n",
    "        ## B: Ablation Study for Different Dataset Sizes\n",
    "\n",
    "        RAND_SEED = 1234\n",
    "\n",
    "        dataset_A_labelled_x_orig = dataset_A_labelled_x.copy()\n",
    "        dataset_A_labelled_y_orig = dataset_A_labelled_y.copy()\n",
    "\n",
    "        for ssratio in [50, 100, 200, 500, 1000]:\n",
    "\n",
    "            set_seeds(RAND_SEED)\n",
    "\n",
    "            idxs_sub = np.random.choice(dataset_A_labelled_x_orig.shape[0], ssratio)\n",
    "\n",
    "            dataset_A_labelled_x = dataset_A_labelled_x_orig[idxs_sub]\n",
    "            dataset_A_labelled_y = dataset_A_labelled_y_orig[idxs_sub]\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"dataset full size: \", all_x.shape)\n",
    "            print(\"\")\n",
    "            print(\"dataset_B_unlabelled_x: \", dataset_B_unlabelled_x.shape)\n",
    "            print(\"dataset_B_unlabelled_y: \", dataset_B_unlabelled_y.shape)\n",
    "            print(\"\")\n",
    "            print(\"dataset_A_labelled_x: \", dataset_A_labelled_x.shape)\n",
    "            print(\"dataset_A_labelled_y: \", dataset_A_labelled_y.shape)\n",
    "            print(\"\")\n",
    "\n",
    "            dataset_B_unlabelled_y_preds_seed_1234 = train_and_predict_test(dataset_A_labelled_x, dataset_A_labelled_y, RUN_ID, N_EPOCHS, IMBALANCE_STRATEGY, dataset_B_unlabelled_x, learning_rate=LEARNING_RATE, dropout_rate=DROPOUT_RATE, batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACC_STEPS, rand_seed=RAND_SEED, language_model=LANGUAGE_MODEL, language_for_model=LANGUAGE_FOR_MODEL, custom_model_name=CUSTOM_MODEL_NAME, do_validation_set=DO_VALIDATION_SET, is_debug=IS_DEBUG_ENABLED, log_with_wandb=IS_LOGGING_ENABLED, wandb_config=wandb_config)\n",
    "            np.savetxt(f\"./output/{RUN_ID}_predictions-1234-sub-{ssratio}.csv\", dataset_B_unlabelled_y_preds_seed_1234, fmt='%f')\n",
    "\n",
    "            compute_and_print_metrics_for_dataset_b(dataset_B_unlabelled_y, [ dataset_B_unlabelled_y_preds_seed_1234 ], None, RAND_SEED, False, True, dataset_name = f'{RUN_ID}_{str(ssratio)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
